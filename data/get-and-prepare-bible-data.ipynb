{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d51cc34",
   "metadata": {},
   "source": [
    "This notebook processes Bambara Bible audio–text pairs extracted from Bible.is for speech and NLP experimentation. \n",
    "\n",
    "We load the scraped chapter data, verifies verse-level alignment, and prepares the material for downstream modeling and analysis.\n",
    "\n",
    "**Data Content:**\n",
    "The extraction includes:\n",
    "\n",
    "* Bambara Bible audio recordings\n",
    "* Corresponding aligned text (verse-level)\n",
    "\n",
    "**Usage Context:**\n",
    "According to Bible.is terms, the content may be used for non-profit, personal, study, or research purposes. The platform explicitly disallows redistribution of the content, or the creation of external services or proxies replicating their materials.\n",
    "\n",
    "**Purpose of Extraction:**\n",
    "The audio-text pairs are used exclusively for research in Bambara Natural Language Processing (NLP), primarily for:\n",
    "\n",
    "* Text-to-Speech (TTS)\n",
    "* Automatic Speech Recognition (ASR)\n",
    "* Audio-Text alignment\n",
    "* Language modeling & data analysis\n",
    "\n",
    "**Non-Commercial Intent:**\n",
    "This material is used solely for private academic experimentation. No content (audio or text) from Bible.is will be redistributed, rehosted, resold, or transformed into a public-facing dataset or service.\n",
    "\n",
    "**Ethical Notes:**\n",
    "The research respects the platform’s non-profit usage constraints and avoids redistribution. Only model performance metrics, analysis, or high-level results may be shared publicly without exposing the original media.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef81e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import aiofiles\n",
    "import aiohttp\n",
    "from playwright.async_api import async_playwright, Locator, Page, Browser\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, wait_random\n",
    "from tqdm.asyncio import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class VerseData:\n",
    "    \"\"\"Data structure for storing verse information.\"\"\"\n",
    "    verse_id: str\n",
    "    text: str\n",
    "\n",
    "@dataclass\n",
    "class ChapterData:\n",
    "    \"\"\"Data structure for storing chapter information.\"\"\"\n",
    "    audio_path: str\n",
    "    non_drama: bool\n",
    "    verses: List[VerseData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d36345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BibleScraper:\n",
    "\n",
    "    \n",
    "    USER_AGENTS = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1\"\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, base_url: str, output_dir: str, max_workers: int = 2, delay_min: float = 5.0, delay_max: float = 12.0):\n",
    "        \"\"\"\n",
    "        Initialize the scraper with configuration parameters.\n",
    "        \n",
    "        Args:\n",
    "            base_url (str): The base URL to start scraping from\n",
    "            output_dir (str): Directory to store downloaded files\n",
    "            max_workers (int): Maximum number of concurrent workers\n",
    "            delay_min (float): Minimum delay between requests in seconds\n",
    "            delay_max (float): Maximum delay between requests in seconds\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.audio_dir = self.output_dir / \"bible_supyire\" / \"audio\"\n",
    "        self.json_dir = self.output_dir / \"bible_supyire\" / \"json\"\n",
    "        self.semaphore = asyncio.Semaphore(max_workers)\n",
    "        self.delay_min = delay_min\n",
    "        self.delay_max = delay_max\n",
    "        \n",
    "\n",
    "        parsed_url = urlparse(base_url)\n",
    "        path_parts = parsed_url.path.split('/')\n",
    "        self.language_code = path_parts[2] if len(path_parts) > 2 else None\n",
    "        \n",
    "\n",
    "        self.audio_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.json_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    async def _random_delay(self):\n",
    "        \"\"\"Add a random delay to avoid detection.\"\"\"\n",
    "        delay = random.uniform(self.delay_min, self.delay_max)\n",
    "        await asyncio.sleep(delay)\n",
    "\n",
    "    def _get_random_user_agent(self):\n",
    "        \"\"\"Get a random user agent from the list.\"\"\"\n",
    "        return random.choice(self.USER_AGENTS)\n",
    "\n",
    "    @retry(\n",
    "        stop=stop_after_attempt(3),\n",
    "        wait=wait_exponential(multiplier=1, min=2, max=5) + wait_random(0, 2)\n",
    "    )\n",
    "    async def _get_chapter_urls(self, page: Page) -> List[str]:\n",
    "        logger.info(\"Collecting chapter URLs...\")\n",
    "        \n",
    "\n",
    "        await page.locator(\"#chapter-dropdown-button\").click()\n",
    "        await asyncio.sleep(2)  \n",
    "        \n",
    "\n",
    "        chapter_elements = await page.locator(\".chapter-container a.chapter-box\").all()\n",
    "        urls = []\n",
    "        for element in chapter_elements:\n",
    "            href = await element.get_attribute(\"href\")\n",
    "            if href:\n",
    "         \n",
    "                if self.language_code and self.language_code in href:\n",
    "                    urls.append(urljoin(self.base_url, href))\n",
    "        \n",
    "        logger.info(f\"Found {len(urls)} chapter URLs for language code {self.language_code}\")\n",
    "        return urls\n",
    "\n",
    "    @retry(\n",
    "        stop=stop_after_attempt(3),\n",
    "        wait=wait_exponential(multiplier=1, min=2, max=5) + wait_random(0, 2)\n",
    "    )\n",
    "    async def _download_audio(self, url: str, audio_src: str, filename: str) -> str:\n",
    "        output_path = self.audio_dir / filename\n",
    "        \n",
    "        if output_path.exists():\n",
    "            return str(output_path)\n",
    "        \n",
    "        headers = {\n",
    "            \"User-Agent\": self._get_random_user_agent(),\n",
    "            \"Referer\": url,\n",
    "            \"Accept\": \"audio/webm,audio/ogg,audio/mp4,audio/*;q=0.9,application/ogg;q=0.7,video/*;q=0.6,*/*;q=0.5\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"DNT\": \"1\",\n",
    "        }\n",
    "        \n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(audio_src, headers=headers) as response:\n",
    "                if response.status == 200:\n",
    "                    async with aiofiles.open(output_path, 'wb') as f:\n",
    "                        await f.write(await response.read())\n",
    "                else:\n",
    "                    raise Exception(f\"Failed to download audio: {response.status}\")\n",
    "        \n",
    "        return str(output_path)\n",
    "\n",
    "    async def _process_chapter(self, browser: Browser, url: str) -> Optional[Dict]:\n",
    "        if self.language_code and self.language_code not in url:\n",
    "            logger.info(f\"Skipping {url} - not matching our language code {self.language_code}\")\n",
    "            return None\n",
    "            \n",
    "        json_filename = f\"{url.split('/')[-2]}_{url.split('/')[-1]}.json\"\n",
    "        json_path = self.json_dir / json_filename\n",
    "        \n",
    "        if json_path.exists():\n",
    "            logger.info(f\"Skipping {url} - already processed\")\n",
    "            return None\n",
    "\n",
    "        page = None\n",
    "        context = None\n",
    "        \n",
    "        try:\n",
    "            context = await browser.new_context(\n",
    "                user_agent=self._get_random_user_agent(),\n",
    "                viewport={\"width\": random.randint(1200, 1600), \"height\": random.randint(800, 1000)},\n",
    "                locale=\"en-US\"\n",
    "            )\n",
    "            \n",
    "\n",
    "            await context.set_extra_http_headers({\n",
    "                \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "                \"DNT\": \"1\",\n",
    "            })\n",
    "            \n",
    "            page = await context.new_page()\n",
    "            \n",
    "            await page.goto(url, wait_until=\"networkidle\")\n",
    "            \n",
    "\n",
    "            await page.evaluate(\"\"\"\n",
    "                window.scrollTo({\n",
    "                    top: Math.random() * 100,\n",
    "                    behavior: 'smooth'\n",
    "                });\n",
    "            \"\"\")\n",
    "            \n",
    "            await self._random_delay()  \n",
    "\n",
    "\n",
    "            audio_player = page.locator(\"video.audio-player\")\n",
    "            if not await audio_player.count():\n",
    "                logger.warning(f\"No audio player found for {url}\")\n",
    "                return None\n",
    "            \n",
    "\n",
    "            audio_src = await audio_player.get_attribute(\"src\")\n",
    "            if not audio_src or audio_src == \"_\":\n",
    "                logger.warning(f\"No audio source for {url}\")\n",
    "                return None\n",
    "            \n",
    "            non_drama = False\n",
    "            audio_drama_toggle = page.locator(\".audio-drama-toggle-container #non-drama-button:not(.disabled)\")\n",
    "            if await audio_drama_toggle.count() > 0:\n",
    "                await audio_drama_toggle.click()\n",
    "                await self._random_delay()  \n",
    "                non_drama = True\n",
    "            \n",
    "            audio_player = page.locator(\"video.audio-player\")\n",
    "            audio_src = await audio_player.get_attribute(\"src\")\n",
    "            if not audio_src or audio_src == \"_\":\n",
    "                logger.warning(f\"No audio source for {url}\")\n",
    "                return None\n",
    "            \n",
    "            filename = f\"{url.split('/')[-2]}_{url.split('/')[-1]}.mp3\"\n",
    "            audio_path = await self._download_audio(url, audio_src, filename)\n",
    "            \n",
    "            await self._random_delay()\n",
    "            \n",
    "            verses = []\n",
    "            verse_elements = await page.locator(\".main-wrapper .chapter span.align-left span[data-verseid]\").all()\n",
    "            \n",
    "            for verse_elem in verse_elements:\n",
    "                verse_id = await verse_elem.get_attribute(\"data-verseid\")\n",
    "                text = await verse_elem.text_content()\n",
    "                verses.append(VerseData(verse_id=verse_id, text=text.strip()))\n",
    "            \n",
    "            data = {\n",
    "                \"audio_path\": audio_path,\n",
    "                \"non_drama\": non_drama,\n",
    "                \"verses\": [{\"verse_id\": v.verse_id, \"text\": v.text} for v in verses]\n",
    "            }\n",
    "            \n",
    "            async with aiofiles.open(json_path, 'w') as f:\n",
    "                await f.write(json.dumps(data, ensure_ascii=False, indent=2))\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {url}: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "        finally:\n",
    "            if page:\n",
    "                await page.close()\n",
    "            if context:\n",
    "                await context.close()\n",
    "\n",
    "            await self._random_delay()\n",
    "\n",
    "    async def scrape(self) -> None:\n",
    "\n",
    "        async with async_playwright() as p:\n",
    "            browser = await p.chromium.launch(\n",
    "                headless=True\n",
    "            )\n",
    "            \n",
    "\n",
    "            context = await browser.new_context(\n",
    "                user_agent=self._get_random_user_agent(),\n",
    "                viewport={\"width\": random.randint(1200, 1600), \"height\": random.randint(800, 1000)}\n",
    "            )\n",
    "            page = await context.new_page()\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                await page.goto(self.base_url, wait_until=\"networkidle\")\n",
    "                await self._random_delay()  # Wait before doing anything\n",
    "                chapter_urls = await self._get_chapter_urls(page)\n",
    "                \n",
    "                \n",
    "                random.shuffle(chapter_urls)\n",
    "                \n",
    "            finally:\n",
    "                await page.close()\n",
    "                await context.close()\n",
    "            \n",
    "\n",
    "            tasks = []\n",
    "            for url in chapter_urls:\n",
    "                tasks.append(self._process_chapter_with_semaphore(browser, url))\n",
    "            \n",
    "            results = await tqdm.gather(*tasks, desc=\"Processing chapters\")\n",
    "            \n",
    "            await self._combine_json_files()\n",
    "            \n",
    "            await browser.close()\n",
    "\n",
    "    async def _process_chapter_with_semaphore(self, browser, url: str) -> Optional[Dict]:\n",
    "        async with self.semaphore:\n",
    "            return await self._process_chapter(browser, url)\n",
    "\n",
    "    async def _combine_json_files(self) -> None:\n",
    "        combined_data = []\n",
    "        \n",
    "        for json_file in self.json_dir.glob(\"*.json\"):\n",
    "            async with aiofiles.open(json_file) as f:\n",
    "                content = await f.read()\n",
    "                combined_data.append(json.loads(content))\n",
    "        \n",
    "        combined_output = self.output_dir / \"combined_bible_data.json\"\n",
    "        async with aiofiles.open(combined_output, 'w') as f:\n",
    "            await f.write(json.dumps(combined_data, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_scraper(base_url=\"https://live.bible.is/bible/SPPTBL/MAT/1\", \n",
    "                     output_dir=\"supyire/data\", \n",
    "                     max_workers=2,\n",
    "                     delay_min=5.0,\n",
    "                     delay_max=12.0):\n",
    "    \"\"\"\n",
    "    Run the scraper with the specified parameters.\n",
    "    \n",
    "    Args:\n",
    "        base_url (str): The base URL to start scraping from\n",
    "        output_dir (str): Directory to store output files\n",
    "        max_workers (int): Maximum number of concurrent workers\n",
    "        delay_min (float): Minimum delay between requests in seconds\n",
    "        delay_max (float): Maximum delay between requests in seconds\n",
    "    \"\"\"\n",
    "    scraper = BibleScraper(\n",
    "        base_url=base_url,\n",
    "        output_dir=output_dir,\n",
    "        max_workers=max_workers,\n",
    "        delay_min=delay_min,\n",
    "        delay_max=delay_max\n",
    "    )\n",
    "    \n",
    "    await scraper.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "585afdd6-e584-4d88-8788-5f1c58b7d56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 20:50:38,421 - INFO - Collecting chapter URLs...\n",
      "2025-03-02 20:50:45,651 - INFO - Found 259 chapter URLs for language code SPPTBL\n",
      "Processing chapters: 100%|██████████| 259/259 [1:03:02<00:00, 14.60s/it]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "await run_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c2556d8-dedc-4b35-9876-4daa99060dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e0767",
   "metadata": {},
   "source": [
    "\n",
    "## **Segmentation / Audio–Text Alignment Description**\n",
    "\n",
    "For verse-level speech processing, the Bambara audio and text pairs were temporally aligned using the **TimestampAudio CLI** tool from Waha ([https://github.com/waha-team/waha-ai-timestamper-cli](https://github.com/waha-team/waha-ai-timestamper-cli)).\n",
    "The tool uses Meta’s **MMS ASR** model to generate timestamp boundaries between the audio signal and the corresponding text spans.\n",
    "\n",
    "### **Purpose**\n",
    "\n",
    "The timestamping step provides aligned segments that are required for:\n",
    "\n",
    "* Supervised ASR training (audio ↔ text supervision)\n",
    "* TTS duration modeling\n",
    "* Dataset inspection and forced alignment analysis\n",
    "* Verse-level slicing (optional)\n",
    "* Time-based quality evaluation\n",
    "\n",
    "### **Method**\n",
    "\n",
    "* Audio files (`.mp3`) and matching text files (`.txt`) were placed in a directory\n",
    "* The tool processed each matched pair\n",
    "* Outputs included:\n",
    "\n",
    "  * **JSON** files with timestamp metadata\n",
    "  * **SRT** subtitle files for visual inspection\n",
    "\n",
    "### **Alignment Strategy**\n",
    "\n",
    "Alignment was performed at verse granularity, using:\n",
    "\n",
    "* MMS ASR multilingual models\n",
    "* Automatic language recognition (no manual language override was required for Bambara)\n",
    "* Default silence-handling settings\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6138a1",
   "metadata": {},
   "source": [
    "\n",
    "## **Audio Splitting After Segmentation**\n",
    "\n",
    "After performing verse-level timestamping with the Waha TimestampAudio tool, the extracted audio segments may vary in length. Some segments can be too long for stable model training, while others may be very short. To standardize segment duration and improve usability for NLP/ASR/TTS tasks, we perform **post-segmentation splitting and extraction** based on the timestamp JSON.\n",
    "\n",
    "### **Purpose of Splitting**\n",
    "\n",
    "* Ensure segments are **within a target duration range** (e.g., 1–30 seconds)\n",
    "* Avoid overlong audio that may hinder ASR or TTS training\n",
    "* Respect natural pauses in speech to **preserve intelligibility**\n",
    "* Generate segments suitable for **batch processing and model ingestion**\n",
    "\n",
    "\n",
    "\n",
    "- **Input**: The JSON file generated by the timestamping tool, containing audio filenames, verse-level timings, and corresponding text.\n",
    "- **Segment Extraction**: Using `pydub`, each verse’s start and end times are used to extract a `.wav` file segment.\n",
    "\n",
    "\n",
    "   * Each segment is saved as a separate `.wav` file.\n",
    "   * A new JSON file is generated, containing the following metadata for each segment:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"audio_filepath\": \"segments/audio_168.1.wav\",\n",
    "    \"text\": \"bambaera  text here\",\n",
    "    \"duration\": 3.72\n",
    "  },\n",
    "  {\n",
    "    \"audio_filepath\": \"segments/audio_168.2.wav\",\n",
    "    \"text\": \"bambara text here\",\n",
    "    \"duration\": 22.38\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "### **Result**\n",
    "\n",
    "* Uniform, intelligible segments aligned with text\n",
    "* Ready for **ASR/TTS model training**, evaluation, or further preprocessing\n",
    "* Progress tracking ensures **efficient batch processing** over large datasets\n",
    "* Optional integration with normalization, phonetic alignment, or forced-alignment pipelines\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5278b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_segments_from_json(json_path: str, audio_dir: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Extract segments from audio based on Waha timestamp JSON and export them as WAV files\n",
    "    with a progress bar.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the segmentation JSON file\n",
    "        audio_dir (str): Directory containing the original audio files\n",
    "        output_dir (str): Directory to save extracted segments and output JSON\n",
    "\n",
    "    Returns:\n",
    "        output_json_path (str): Path to the JSON file containing all segment info\n",
    "    \"\"\"\n",
    "\n",
    "    json_path = Path(json_path)\n",
    "    audio_dir = Path(audio_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    segments_info = []\n",
    "\n",
    "    for entry in tqdm(data, desc=\"Processing audio files\", unit=\"file\"):\n",
    "        audio_file = audio_dir / entry[\"audio_file\"]\n",
    "        if not audio_file.exists():\n",
    "            print(f\"Warning: Audio file {audio_file} not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        sections = entry.get(\"sections\", [])\n",
    "\n",
    "        for section in tqdm(sections, desc=f\"Processing sections in {entry['audio_file']}\", leave=False, unit=\"segment\"):\n",
    "            start_sec, end_sec = section[\"timings\"]\n",
    "            text = section[\"text\"]\n",
    "\n",
    "            segment_filename = f\"{section['verse_id']}.wav\"\n",
    "            segment_path = output_dir / segment_filename\n",
    "\n",
    "            audio = AudioSegment.from_file(audio_file)\n",
    "            start_ms = int(start_sec * 1000)\n",
    "            end_ms = int(end_sec * 1000)\n",
    "            segment_audio = audio[start_ms:end_ms]\n",
    "            segment_audio.export(segment_path, format=\"wav\")\n",
    "\n",
    "            segments_info.append({\n",
    "                \"audio_filepath\": str(segment_path),\n",
    "                \"text\": text,\n",
    "                \"duration\": len(segment_audio) / 1000.0\n",
    "            })\n",
    "\n",
    "    output_json_path = output_dir / f\"{json_path.stem}_segments.json\"\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(segments_info, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Exported {len(segments_info)} segments to {output_json_path}\")\n",
    "    return str(output_json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd4421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json = create_segments_from_json(\n",
    "    json_path=\"segments_timestamp.json\",\n",
    "    audio_dir=\"audio/\",\n",
    "    output_dir=\"segments/\"\n",
    ")\n",
    "\n",
    "with open(output_json, \"r\") as f:\n",
    "    segments = json.load(f)\n",
    "    \n",
    "print(segments[:2])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
